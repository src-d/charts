apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "lookout-style-analyzer.fullname" . }}
  labels:
    app: {{ template "lookout-style-analyzer.name" . }}
    chart: {{ template "lookout-style-analyzer.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    metadata:
      name: {{ template "lookout-style-analyzer.fullname" . }}
      labels:
        app: {{ template "lookout-style-analyzer.name" . }}
        chart: {{ template "lookout-style-analyzer.chart" . }}
        release: {{ .Release.Name }}
        heritage: {{ .Release.Service }}
    spec:
      restartPolicy: Never
      volumes:
        - name: tmp-pod
          emptyDir: {}
        - name: data
          {{- if .Values.app.volume.hostPath }}
          hostPath:
            path: {{ .Values.app.volume.hostPath }}
          {{- else }}
          persistentVolumeClaim:
            claimName: {{ template "lookout-style-analyzer.fullname" . }}
          {{- end }}
        {{- if .Values.databases.postgres.cloudSQL }}
        - name: cloudsql-instance-credentials
          secret:
            secretName: {{ required "missing databases.postgres.serviceAccountSecret" .Values.databases.postgres.serviceAccountSecret }}
        {{- end }}
      containers:
      {{- if .Values.databases.postgres.cloudSQL }}
        - name: cloudsql-proxy
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          command: ["/bin/sh", "-c"]
            # The following is a hack needed as Kubernetes wants the 2 containers to exit to complete the job
            # See https://github.com/kubernetes/kubernetes/issues/25908 
          args:
          - |
            /cloud_sql_proxy --dir=/cloudsql -instances={{ required "missing databases.postgres.instanceConnectionName" .Values.databases.postgres.instanceConnectionName }}=tcp:5432 -credential_file=/secrets/cloudsql/credentials.json &
            CHILD_PID=$!
            (while true; do if [[ -f "/tmp/pod/main-terminated" ]]; then kill $CHILD_PID; echo "Killed $CHILD_PID as the main container terminated."; fi; sleep 1; done) &
            wait $CHILD_PID
            if [[ -f "/tmp/pod/main-terminated" ]]; then exit 0; echo "Job completed. Exiting..."; fi
          securityContext:
            runAsUser: 2  # non-root user
            allowPrivilegeEscalation: false
          volumeMounts:
            - name: cloudsql-instance-credentials
              mountPath: /secrets/cloudsql
              readOnly: true
            - name: tmp-pod
              mountPath: /tmp/pod
      {{- end }}
        - name: initdb
          image: "{{ .Values.image.repository }}:{{ required "Missing image.tag" .Values.image.tag  }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: ["/bin/sh", "-c"]
          args:
            - |
              trap "touch /tmp/pod/main-terminated" EXIT
              python3 lookout init --db $(LOOKOUT_DB) --fs /tmp/models
          env:
            - name: LOOKOUT_DB
              {{- if .Values.databases.postgres.cloudSQL }}
              valueFrom:
                secretKeyRef:
                  name: {{ required "Missing databases.postgres.connectionDetailsSecret" .Values.databases.postgres.connectionDetailsSecret }}
                  key: LOOKOUT_DB
              {{- else }}
              value: {{ required "missing databases.postgres.dbConnectionString" .Values.databases.postgres.dbConnectionString }}
              {{- end }}
          volumeMounts:
            - name: data
              mountPath: /tmp/models
            - name: tmp-pod
              mountPath: /tmp/pod
    {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.affinity }}
      affinity:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
    {{- end }}
